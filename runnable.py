# -*- coding: utf-8 -*-
"""Copy of MLa2_QUIMHM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ox6ZC8aZzw8uCXgCQlUnVDcBv5oREhpd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve,roc_auc_score

import tensorflow as tf
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

# Load dataset
dataset = pd.read_csv('/train-io.txt', sep=' ', header=None)
# Inspect data
dataset.head()

# Print summary statistics
print(dataset.describe())
# Print DataFrame information
print(dataset.describe().info())

# Segregate features and labels into separate variables
X, y = dataset.iloc[:, 0: 10].values, dataset.iloc[:,10].values

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 69)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

col_count = X.shape[1]
activator = 'relu'
nodes = 1000 
max_layers = 2
max_epochs = 50
max_batch = 256
loss_funct = 'binary_crossentropy' 
last_act = 'sigmoid'
adam = tf.keras.optimizers.Adam(learning_rate=0.001)

def baseline_model():
    # create model
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(nodes, input_dim=col_count, activation=activator))
    for x in range(0, max_layers):
        #model.add(Dropout(0.2))
        model.add(tf.keras.layers.Dense(nodes, input_dim=nodes, activation=activator))    
    #model.add(BatchNormalization())
    model.add(tf.keras.layers.Dense(1, activation=last_act))
    # Compile model
    model.compile(loss=loss_funct, optimizer=adam, metrics=['accuracy'])
    return model

estimator = KerasClassifier(build_fn=baseline_model, epochs=max_epochs, batch_size=max_batch)
estimator.fit(X_train,y_train)

y_pred = estimator.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
score = np.sum(cm.diagonal())/float(np.sum(cm))
print(score)

sum=0
total=0
for y1,y2 in zip(y_test,y_pred):
  if y1==y2: sum+=1
  total+=1
print(sum, sum/total)

y_pred = estimator.predict(X_train)
sum=0
total=0
for y1,y2 in zip(y_train,y_pred):
  if y1==y2: sum+=1
  total+=1
print(sum, sum/total)

X_assessment = pd.read_csv('/test-i.txt', sep=' ', header=None)
X_assessment = sc.transform(X_assessment)
y_pred = estimator.predict(X_assessment)
print(y_pred)

np.savetxt("/test-o.txt", y_pred, delimiter="\n",fmt='%d')

#MIXTURE OF EXPERTS (NAIVE IMPLEMENTATION)
"""
n_e = 3
experts = []
max_epochs = 75
max_batch = 256
nodes = 1000//n_e
for i in range(n_e):
  inputX = X_train[i*len(X_train)//n_e:((i+1)*len(X_train)//n_e)-1]
  inputY = y_train[i*len(y_train)//n_e:((i+1)*len(y_train)//n_e)-1]
  experts.append(KerasClassifier(build_fn=baseline_model, epochs=max_epochs, batch_size=max_batch))
  experts[i].fit(inputX,inputY)

y1 = experts[0].predict(X_test)
y2 = experts[1].predict(X_test)
y3 = experts[2].predict(X_test)
y_pred = (y1+y2+y3>1.5)

sum=0
total=0
for y1,y2 in zip(y_test,y_pred):
  if y1==y2: sum+=1
  total+=1
print(sum, sum/total)
"""
